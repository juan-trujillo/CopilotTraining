# The Agentic Journey: Incremental Path from Issue to PR Automation

> **The Question This Talk Answers:**
> *"How do I start getting value from GitHub Copilot agents without rewiring my entire SDLC?"*

**Duration:** 45 minutes | **Target Audience:** Developers / DevOps / Engineering Managers

---

## ğŸ“Š Content Fitness

| Criterion | Assessment | Notes |
|-----------|-----------|-------|
| **Relevant** | ğŸŸ¢ High | Every team using GitHub has issues and PRs â€” this automates the lifecycle with zero repo restructuring |
| **Compelling** | ğŸŸ¢ High | Working workflow files you can drop into any repo today; progressive 4-phase approach avoids big-bang risk |
| **Actionable** | ğŸŸ¢ High | Copy 4 YAML files, configure secrets, create an issue â€” agents start working in under 3 hours |

**Overall Status:** ğŸŸ¢ Ready to use

---

## ğŸ“½ï¸ Slide Generation Mapping

### Slide Sequence (Generated Automatically)

1. **Title/Logo Slide** â† H1 title + subtitle
2. **Question/Objective Slide** â† "The Question This Talk Answers"
3. **Table of Contents Slide** â† Auto-generated from ğŸ¬ sections
4. **Problem Slide** â† "The Problem"
5. **Solution Overview** â† "The Solution"
6. **Key Artifacts** â† "Key Artifacts" inventory
7. **Mental Model Shift** â† Move-Toward/Away/Against
8. **When to Use Decision Tree** â† "When to Use This Pattern"
9. **Phase 1: Agentic Intake** â† ğŸ¬ Section 1 (3-4 slides)
10. **Phase 2: Agentic Planning** â† ğŸ¬ Section 2 (3-4 slides)
11. **Phase 3: Agentic Coding** â† ğŸ¬ Section 3 (2-3 slides)
12. **Phase 4: Code Review** â† ğŸ¬ Section 4 (3-4 slides)
13. **Graduation to Full SDLC** â† ğŸ¬ Section 5 (2-3 slides)
14. **Use Cases** â† Real-World Use Cases (1-2 slides)
15. **Actionable Outcomes** â† What You Can Do Today
16. **Related Patterns** â† Related Patterns
17. **Official Documentation** â† ğŸ“š section
18. **End Slide** â† Auto-generated

### Major Sections (TOC Entries)

```markdown
<!-- ğŸ¬ MAJOR SECTION: Agentic Intake -->
<!-- ğŸ¬ MAJOR SECTION: Agentic Planning -->
<!-- ğŸ¬ MAJOR SECTION: Agentic Coding -->
<!-- ğŸ¬ MAJOR SECTION: Code Review -->
<!-- ğŸ¬ MAJOR SECTION: Graduating to Full SDLC -->
```

---

## The Problem

### Key Points

- **Every issue requires manual investigation**
  Duplicate detection, type classification, priority assessment, and team routing cost 20-30 minutes per issue.

- **Planning is expensive and error-prone**
  Developers spend 2-4 hours researching codebase, identifying files, and estimating effort â€” often without learning from similar past work.

- **Code review is the new bottleneck**
  Agent-generated code arrives faster than humans can review it â€” 500-2000 lines per PR, 10-15x volume increase.

- **Adoption paralysis: "all in" vs. "experiment forever"**
  Teams either attempt full SDLC transformation (and fail) or stay in perpetual pilot mode (and stall).

### Narrative

You've heard about Gen-4 SDLC. You've read about agentic repositories, PR workflows, and CI trust factories. It sounds transformative â€” and also overwhelming.

Most organizations try to adopt AI development in one of two ways, both of which fail. The "all in" approach rewrites CI/CD from scratch, restructures repos, and changes all team processes in a matter of weeks â€” then everything breaks and rolls back. The "experiment forever" approach pilots with one team per quarter, debates results endlessly, and ends up still in pilot mode after a year.

The better path is incremental. Start with the highest-ROI, lowest-risk workflow â€” automating issue intake â€” and prove value in the first week. Add planning agents in week two. Layer in code review automation by month two. Each phase compounds the previous one. And when you hit limits â€” when coordination overhead exceeds automation gains â€” you're ready for the full SDLC transformation with proof of value and executive support already in hand.

---

## The Solution: 4-Workflow Issue Lifecycle

### What It Does

Four GitHub Actions workflows coordinate via issue labels to automate the complete issue-to-PR lifecycle. Each phase operates independently, triggered by label state transitions, creating a clean separation of concerns that scales from a single repo to an organization.

### Key Capabilities

- **Agentic Intake**: Automatic triage, duplicate detection, context gathering, and team routing on every new issue (20-30 min â†’ 3 min)
- **Agentic Planning**: Historical context research, codebase analysis, and execution plan generation with human approval gate (2-4 hours â†’ 15 min)
- **Agentic Coding**: Implementation from approved plans with test generation and PR creation (8-12 hours â†’ 1-2 hours)
- **Code Review**: Security analysis, logic review, performance evaluation, and test coverage assessment before human review (2-4 hours â†’ 20 min)

### Architecture Overview

The system uses a **label-based state machine** where each workflow adds a status label that triggers the next phase. This creates a clean, observable pipeline: `issue opened` â†’ `status:triaged` â†’ `status:planned` â†’ `/approve-plan` â†’ `status:in-review` â†’ `status:reviewed` â†’ human approval.

All agent instructions are **externalized as markdown prompt templates** in `.github/prompts/`, keeping workflow YAML focused on orchestration. This means updating agent behavior requires editing a markdown file, not debugging YAML â€” and non-technical stakeholders can review and improve prompt templates directly.

The architecture deliberately separates the "when to run" (workflow triggers) from the "what to do" (prompt templates), making each concern independently versioned, testable, and maintainable.

**Official Documentation:**
- ğŸ“– [GitHub Copilot Documentation](https://docs.github.com/en/copilot) â€” Core concepts and getting started
- ğŸ“– [GitHub Actions Workflow Syntax](https://docs.github.com/en/actions/reference/workflow-syntax-for-github-actions) â€” Workflow triggers, jobs, and steps
- ğŸ“– [Copilot for Pull Requests](https://github.com/features/copilot/pull-requests) â€” Agent-assisted PR workflows

---

## ğŸ“¦ Key Artifacts

**This talk includes 4 complete, working GitHub Actions workflow files** that implement the full issue-to-PR lifecycle. Each is shown inline in its corresponding major section below.

### Primary Artifacts

*Shown inline with detailed explanation in the major sections*

- **[`1-issue-triage.yml`](1-issue-triage.yml)** â€” Automated intake: duplicate detection, classification, routing, context summary
- **[`2-issue-planning.yml`](2-issue-planning.yml)** â€” Historical context research, codebase analysis, execution plan generation
- **[`3-issue-execution.yml`](3-issue-execution.yml)** â€” Implementation from approved plan, test generation, PR creation
- **[`4-pr-review.yml`](4-pr-review.yml)** â€” Security analysis, logic review, performance evaluation, test coverage

### Supporting Files

*Referenced but not shown inline â€” available in repository*

- **[`.github/prompts/`](../../.github/prompts/)** â€” Externalized prompt templates for all 4 workflows
- **[Agentic SDLC (Full Guide)](../agentic-sdlc/)** â€” Complete Gen-4 transformation when ready to graduate

### Externalized Prompt Architecture

All workflows load instructions from `.github/prompts/` using `sed` for variable substitution:

```bash
sed -e "s|{{ISSUE_NUMBER}}|${ISSUE_NUM}|g" \
    .github/prompts/triage-instructions.md > instructions.txt
copilot -p @instructions.txt
```

**Prompt files:**
```
.github/prompts/
â”œâ”€â”€ triage-instructions.md        # Phase 1 agent behavior
â”œâ”€â”€ planning-instructions.md      # Phase 2 agent behavior
â”œâ”€â”€ implementation-instructions.md # Phase 3 agent behavior
â”œâ”€â”€ review-instructions.md        # Phase 4 agent behavior
â””â”€â”€ README.md                     # Complete documentation
```

**Benefits:** Easy maintenance (edit markdown, not YAML), version control, non-technical collaboration, reusability across workflows, and prompts serve as documentation of agent behavior.

---

## ğŸ¯ Mental Model Shift

> **The Core Insight:** From "humans drive every step of the issue lifecycle" to "agents handle the routine, humans govern the outcomes"

### Move Toward (Embrace These Patterns)

- âœ… **Label-Driven Orchestration**: Use labels as state transitions between workflow phases â†’ Clean separation, observable pipeline, easy debugging
- âœ… **Externalized Prompts**: Store agent instructions in markdown files, not embedded in YAML â†’ Version-controlled, reviewable, maintainable prompt evolution
- âœ… **Historical Context**: Agents research past similar issues and PRs before planning â†’ 60% better estimate accuracy, 40% fewer mid-implementation surprises
- âœ… **Incremental Adoption**: Deploy one phase at a time, prove value, then expand â†’ Build momentum with evidence, not faith

### Move Away From (Retire These Habits)

- âš ï¸ **Manual Triage**: Developers spending 20-30 minutes classifying, routing, and duplicate-checking every issue â†’ Context-switch cost exceeds triage value
- âš ï¸ **Planning from Scratch**: Starting every implementation plan without consulting past work â†’ Misses patterns, underestimates effort, repeats mistakes
- âš ï¸ **Line-by-Line Review**: Reading every line of agent-generated PRs the same way you review human code â†’ Can't scale to 10-15x volume increase

### Move Against (Active Resistance Required)

- ğŸ›‘ **All-or-Nothing Transformation**: Attempting full Gen-4 SDLC restructuring before proving value with quick wins â†’ Rollback risk, team fatigue, lost credibility
- ğŸ›‘ **Embedded Prompt Instructions**: Hardcoding agent behavior inside workflow YAML â†’ Unmaintainable, unreviewable, impossible to iterate on

> **Example Transformation:** Before: Issue #4821 "Login fails on mobile Safari" â€” developer spends 45 minutes researching, discovers it's a duplicate of #4203 fixed 2 weeks ago. After: Triage agent identifies 88% similarity to #4203 in 3 minutes, auto-labels as duplicate, posts fix version. Savings: 40 minutes per duplicate (14 duplicates/month in this repo).

---

## When to Use This Pattern

### Decision Tree

```
Q: Where are you in your agentic adoption journey?
â”œâ”€ "Haven't started yet"
â”‚  â†’ Start here with Phase 1 (Agentic Intake)
â”‚  â””â”€ Best for: Any team, any repo, 2-3 hours setup
â”‚
â”œâ”€ "Already assigning issues to Copilot but want more automation"
â”‚  â†’ Add Phases 2 + 4 (Planning + Code Review)
â”‚  â””â”€ Best for: Teams with 20+ issues/month
â”‚
â”œâ”€ "Hitting coordination limits across repos"
â”‚  â†’ Graduate to Full SDLC: See Agentic SDLC talk
â”‚  â””â”€ Best for: Teams with agents touching 2+ repos per feature
â”‚
â””â”€ "Already have full Gen-4 SDLC"
   â†’ See: Enterprise Patterns talk
   â””â”€ Best for: Organization-wide scaling
```

### Use This Pattern When

- You want agentic value without restructuring repos or CI/CD
- Your team processes 20+ issues/month
- You need to prove ROI before requesting larger transformation budget
- Developers spend significant time on triage, planning, or review

### Don't Use This Pattern When

- Agents already touch 2+ repos for >30% of features â†’ Use [Agentic SDLC](../agentic-sdlc/) monorepo approach
- CI pipeline takes >60 minutes â†’ Fix CI first, or see [Agentic SDLC Part 3](../agentic-sdlc/)
- You have <5 issues/month â†’ Manual handling may be fine

### Comparison with Related Features

| Aspect | This Talk (Agentic Journey) | Agentic SDLC | Agentic Sessions |
|--------|----------------------------|--------------|------------------|
| **Best For** | Getting started, proving value | Full transformation | Interactive agent workflows |
| **Setup Time** | 2-3 hours | 3-6 months | Minutes per session |
| **Scope** | Issue lifecycle automation | Repo topology + CI + PRs | Single dev session |
| **Risk Level** | Low (additive, no restructuring) | High (organizational change) | None (developer tool) |

---

<!-- ğŸ¬ MAJOR SECTION: Agentic Intake -->
## Phase 1: Agentic Intake

*Agents analyze, classify, and route issues automatically*

### The Problem

Every new issue requires investigation: Is this a bug or feature? Has it been reported before? Who should own it? What's the priority? Where in the codebase is this?

**Time cost:** 20-30 minutes per issue. **Opportunity cost:** Developers context-switch away from feature work.

### How It Works

The triage workflow triggers on every new issue and performs four steps in ~3 minutes:

1. **Context Gathering** â€” Reads issue title, description, related files, recent similar issues
2. **Duplicate Detection** â€” Searches open and closed issues from last 6 months, calculates similarity
3. **Routing & Labeling** â€” Classifies type, priority, affected component, suggests assignee
4. **Context Summary** â€” Generates analysis for the assignee with affected files, root cause hypothesis, and suggested approach

### The Workflow

```yaml
# .github/workflows/1-issue-triage.yml
name: "Phase 1: Agentic Intake"

on:
  issues:
    types: [opened]

jobs:
  triage:
    runs-on: ubuntu-latest
    permissions:
      issues: write
      contents: read
      pull-requests: read
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: '20'
      - run: npm install -g @github/copilot

      - name: Get issue details
        id: issue
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          gh api /repos/${{ github.repository }}/issues/${{ github.event.issue.number }} > issue.json
          TITLE=$(jq -r '.title' issue.json)
          echo "title=$TITLE" >> $GITHUB_OUTPUT

      - name: Search for duplicate issues
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          TITLE="${{ steps.issue.outputs.title }}"
          # Search open issues
          gh api --method GET /search/issues \
            -f q="repo:${{ github.repository }} is:issue is:open \"${TITLE}\"" \
            --jq '.items[0:5]' > open-issues.txt
          # Search closed issues from last 6 months
          gh api --method GET /search/issues \
            -f q="repo:${{ github.repository }} is:issue is:closed created:>=$(date -d '6 months ago' +%Y-%m-%d) \"${TITLE}\"" \
            --jq '.items[0:5]' > closed-issues.txt

      - name: Run Copilot CLI for triage
        env:
          GITHUB_TOKEN: ${{ secrets.COPILOT_GITHUB_TOKEN }}
        run: |
          # Load externalized prompt template with variable substitution
          sed -e "s|{{ISSUE_NUMBER}}|${{ github.event.issue.number }}|g" \
              -e "s|{{ISSUE_TITLE}}|${{ steps.issue.outputs.title }}|g" \
              .github/prompts/triage-instructions.md > triage_instructions.txt
          copilot -p @triage_instructions.txt > triage_result.txt

      - name: Add labels and post results
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Parse structured output, apply labels, post comment
          # Adds: type label, priority label, status:triaged
```

> ğŸ“ Full workflow: [`1-issue-triage.yml`](1-issue-triage.yml) (129 lines)

### Prompt Template Design

The triage prompt uses structured output format for reliable parsing:

```markdown
## Required Output Format

DUPLICATE: [yes/no] - [issue number if duplicate, otherwise "none"]
TYPE: [bug/feature/documentation/question/other]
PRIORITY: [critical/high/medium/low]
AREA: [affected component]
ASSIGN_TO: [team or username suggestion]
SUMMARY: [2-3 sentence context summary]
```

Structured output ensures consistent label application and automated routing regardless of issue content.

### Metrics

| Metric | Before Agents | After | Improvement |
|--------|---------------|-------|-------------|
| Time to first triage | 30 min | <5 min | 6x faster |
| Duplicate detection rate | 60% caught | >90% caught | 50% improvement |
| Correct initial routing | 60% | >90% | Fewer bounces |
| Context-gathering time | 45 min | <10 min | 4.5x faster |

**ROI:** At 50 issues/month, saves ~20 hours/month (2.5 developer days). Setup: 2-3 hours one-time.

---

<!-- ğŸ¬ MAJOR SECTION: Agentic Planning -->
## Phase 2: Agentic Planning

*Agents research codebase and historical context to generate execution plans*

### The Problem

After triage, someone needs to understand requirements, research the codebase, identify affected files, design the approach, and estimate effort. **Time cost:** 2-4 hours for non-trivial features. **Risk:** Incomplete research leads to mid-implementation surprises.

### The Key Innovation: Historical Context

The planning agent doesn't just analyze the current issue â€” it searches for similar past issues and their associated PRs:

```
Agent searches for similar past issues and PRs:
- "Add Google OAuth" (#2847) â†’ PR #2851: 6 hours, 8 files
  Lesson: Token refresh edge case required extra test
- "Add GitHub OAuth" (#3104) â†’ PR #3109: 5 hours, 7 files
  Lesson: Multi-tenant config needed for enterprise users
```

Plans informed by historical data show **Â±20% estimate accuracy** vs. **Â±50% without** â€” and surface edge cases from previous implementations before coding begins.

### The Workflow

```yaml
# .github/workflows/2-issue-planning.yml
name: "Phase 2: Agentic Planning"

on:
  issues:
    types: [labeled]

jobs:
  planning:
    if: github.event.label.name == 'status:triaged'
    runs-on: ubuntu-latest
    permissions:
      issues: write
      contents: read
      pull-requests: read
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: '20'
      - run: npm install -g @github/copilot

      - name: Search for historical similar issues
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          TITLE=$(cat issue_title.txt)
          gh api --method GET /search/issues \
            -f q="repo:${{ github.repository }} is:issue is:closed created:>=$(date -d '6 months ago' +%Y-%m-%d) ${TITLE}" \
            --jq '.items[0:5]' > similar-issues.txt

      - name: Retrieve PRs from similar issues
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          gh api --method GET /search/issues \
            -f q="repo:${{ github.repository }} is:pr is:merged created:>=$(date -d '6 months ago' +%Y-%m-%d)" \
            --jq '.items[0:5]' > related-prs.txt

      - name: Generate execution plan with Copilot CLI
        env:
          GITHUB_TOKEN: ${{ secrets.COPILOT_GITHUB_TOKEN }}
        run: |
          sed -e "s|{{ISSUE_NUMBER}}|${{ github.event.issue.number }}|g" \
              -e "s|{{SIMILAR_ISSUES}}|$(cat similar-issues.txt)|g" \
              -e "s|{{RELATED_PRS}}|$(cat related-prs.txt)|g" \
              .github/prompts/planning-instructions.md > planning_instructions.txt
          copilot -p @planning_instructions.txt > execution_plan.txt

      - name: Post plan and await approval
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Post execution plan as comment
          # Prompt: "Comment /approve-plan to proceed"
          # Adds label: status:planned
```

> ğŸ“ Full workflow: [`2-issue-planning.yml`](2-issue-planning.yml) (155 lines)

### Example Plan Output

```
ğŸ“‹ EXECUTION PLAN: Add Microsoft OAuth Support

Historical Context:
Based on Issue #2847 "Add Google OAuth" (6h, 8 files) and
Issue #3104 "Add GitHub OAuth" (5h, 7 files):
âœ“ Token refresh edge cases require explicit testing
âœ“ Multi-tenant configuration needed for enterprise
âœ“ Rate limiting should be considered from start

Affected Files:
1. src/auth/providers/microsoft-oauth.ts (new)
2. src/auth/oauth-handler.ts (modify)
3. src/config/auth-config.ts (modify)
4. tests/integration/auth/microsoft-oauth.test.ts (new)

Dependencies: @azure/msal-node@^2.6.0 (security: clean, license: MIT)

Estimated Effort: 6-8 hours (based on historical average)
Risk Level: Medium | Rollback: Feature flag MS_OAUTH_ENABLED
```

### Human Approval Gate

The plan posts as an issue comment and waits for `/approve-plan`. This is the critical human checkpoint â€” agents plan, humans decide.

### Metrics

| Metric | Before Agents | After | Improvement |
|--------|---------------|-------|-------------|
| Time to implementation plan | 4 hours | <30 min | 8x faster |
| Plan accuracy (estimate vs actual) | Â±50% | Â±20% | 60% more accurate |
| Missing requirements found late | 25% | <10% | Less rework |
| Historical context utilization | 0% | >80% | Learn from past work |

---

<!-- ğŸ¬ MAJOR SECTION: Agentic Coding -->
## Phase 3: Agentic Coding

*Agents implement the approved plan, run tests, and create PRs*

### The Good News

**You already have this.** Assigning issues to `copilot-swe-agent[bot]` enables implementation. This workflow formalizes the process by connecting it to the approved plan from Phase 2.

### The Workflow

```yaml
# .github/workflows/3-issue-execution.yml
name: "Phase 3: Agentic Coding"

on:
  issue_comment:
    types: [created]

jobs:
  execution:
    if: |
      github.event.issue.pull_request == null &&
      contains(github.event.comment.body, '/approve-plan')
    runs-on: ubuntu-latest
    permissions:
      issues: write
      contents: write
      pull-requests: write
    steps:
      - name: Verify status:planned label
        run: |
          LABELS=$(gh api /repos/$REPO/issues/$NUM/labels --jq '.[].name')
          echo "$LABELS" | grep -q "status:planned" || exit 1

      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - run: npm install -g @github/copilot

      - name: Implement solution with Copilot CLI
        env:
          GITHUB_TOKEN: ${{ secrets.COPILOT_GITHUB_TOKEN }}
        run: |
          sed -e "s|{{ISSUE_NUMBER}}|${{ github.event.issue.number }}|g" \
              .github/prompts/implementation-instructions.md > impl.txt
          # Append execution plan from Phase 2 comment
          copilot -p @impl.txt > implementation_log.txt

      - name: Create PR
        run: |
          git checkout -b "copilot/issue-${{ github.event.issue.number }}"
          git add . && git commit -m "Implement #${{ github.event.issue.number }}"
          git push origin HEAD
          gh pr create --title "Fix #$NUM" --body "Evidence bundle..."
          # Adds label: status:in-review
```

> ğŸ“ Full workflow: [`3-issue-execution.yml`](3-issue-execution.yml) (179 lines)

### Implementation Guidelines (from prompt template)

The implementation prompt enforces quality standards:
- Follow existing code style and patterns
- Write tests for all new functionality
- Make small, atomic commits with descriptive messages
- Keep functions focused and single-purpose
- Ensure proper error handling

### Metrics

| Metric | Before Agents | After | Improvement |
|--------|---------------|-------|-------------|
| Implementation time | 8-12 hours | 1-2 hours | 6-10x faster |
| Time to first PR | 1-2 days | 2-4 hours | 4-8x faster |
| Initial test pass rate | 70% | >90% | Higher quality |
| PRs requiring rework | 30% | <15% | Less wasted effort |

---

<!-- ğŸ¬ MAJOR SECTION: Code Review -->
## Phase 4: Code Review Agent

*Agents pre-review PRs for security, logic, performance, and test coverage*

### The Problem

Even with agent-generated code, humans must review. But agent volume is 10-15x traditional â€” and each PR touches 500-2000 lines. **Code review becomes the bottleneck on velocity.**

### The Workflow

```yaml
# .github/workflows/4-pr-review.yml
name: "Phase 4: PR Code Review"

on:
  pull_request:
    types: [opened, synchronize, reopened]

jobs:
  code-review:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: write
      issues: write
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
          ref: ${{ github.event.pull_request.head.ref }}

      - run: npm install -g @github/copilot

      - name: Analyze PR changes
        run: |
          git diff origin/${{ github.event.pull_request.base.ref }}...HEAD > pr_changes.diff
          FILES_CHANGED=$(git diff --name-only origin/$BASE...HEAD | wc -l)

      - name: Run security scan
        run: |
          # Check for hardcoded secrets, SQL injection, XSS patterns
          # Report severity levels: Critical/High/Medium/Low

      - name: Run Copilot CLI code review
        env:
          GITHUB_TOKEN: ${{ secrets.COPILOT_GITHUB_TOKEN }}
        run: |
          sed -e "s|{{PR_NUMBER}}|${{ github.event.pull_request.number }}|g" \
              .github/prompts/review-instructions.md > review_instructions.txt
          copilot -p @review_instructions.txt < pr_changes.diff > code_review.txt

      - name: Post comprehensive review
        run: |
          # Post structured review: Security + Logic + Performance + Tests
          # Adds labels: status:reviewed, agent-generated
          # If critical issues: adds needs-attention label
```

> ğŸ“ Full workflow: [`4-pr-review.yml`](4-pr-review.yml) (218 lines)

### What the Agent Reviews

The review prompt covers four focus areas with structured output:

| Focus Area | What It Checks | Time |
|------------|---------------|------|
| **Security** | SQL injection, XSS, auth bypasses, hardcoded secrets, unsafe dependencies | 2 min |
| **Logic** | Error handling, edge cases, race conditions, resource leaks | 2 min |
| **Performance** | O(nÂ²) algorithms, N+1 queries, blocking operations, caching opportunities | 1 min |
| **Tests** | Coverage on new code, edge case tests, integration tests, flaky test detection | 1 min |

Human review shifts from "read every line" to **"validate outcomes against intent"** â€” does the implementation match the approved plan?

### Metrics

| Metric | Before Agents | After | Improvement |
|--------|---------------|-------|-------------|
| Time per PR review | 2-4 hours | 15-20 min | 8-12x faster |
| Critical bugs caught pre-merge | 60% | >95% | Near-complete |
| Security issues in production | 5/quarter | 0/quarter | Eliminated |
| Review bottleneck | Yes | No | Unblocked |

---

<!-- ğŸ¬ MAJOR SECTION: Graduating to Full SDLC -->
## Phase 5: When to Graduate to Full SDLC

*Signals that you've outgrown quick wins and need Gen-4 transformation*

### The Inflection Point

Phases 1-4 work brilliantly â€” until they don't. Watch for these signals:

| Signal | Symptom | Solution |
|--------|---------|----------|
| **Repository Chaos** | Agents touching 3-5 repos per feature | Monorepo consolidation ([Agentic SDLC Part 1](../agentic-sdlc/)) |
| **CI Bottleneck** | CI queue time >60 minutes | Trust factory CI ([Agentic SDLC Part 3](../agentic-sdlc/)) |
| **Review Overwhelm** | 50+ PRs open, 10+ days to review | Outcome-based PRs ([Agentic SDLC Part 2](../agentic-sdlc/)) |
| **Test Flakiness** | >10% flaky test rate | Hermetic builds, deterministic signal |
| **Manual Governance** | Compliance requires human review | Automated evidence bundles |

### The Upgrade Path

```
PHASE 1-4 (Quick Wins)              â†’    PHASE 5 (Full SDLC)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ“ Issue â†’ PR automation                   âœ“ Monorepo with module boundaries
âœ“ Agent-generated code                    âœ“ Feature-scale PRs (500-2000 lines)
âœ“ Code review assistance                  âœ“ Intent-based reviews
~ Manual coordination                     âœ“ Atomic merges
~ Traditional CI (slow)                   âœ“ CI as trust factory (<10 min)

THROUGHPUT: 5-10 features/week       â†’    THROUGHPUT: 10-15 features/day
```

### Graduation Criteria

| Criterion | Threshold | How to Measure |
|-----------|-----------|----------------|
| Agent velocity | >10 PRs/week | GitHub Insights |
| Human bottleneck | Review queue >5 days | PR aging reports |
| Multi-repo friction | >30% PRs touch 2+ repos | PR analysis |
| CI capacity | Queue time >30 min peak | Actions metrics |

---

## Real-World Use Cases

### Use Case 1: Duplicate Issue Elimination

**The Problem:** A team with 50 issues/month had a 15% duplicate rate. Developers spent 45 minutes per duplicated issue researching before realizing it was already fixed.

**The Solution:** Phase 1 triage agent searches open and closed issues, calculates similarity, and auto-labels duplicates with references to the original fix.

**Outcome:** Duplicate rate dropped from 15% to <3%. Saved 14 Ã— 40 minutes = **9.3 hours/month** on duplicates alone.

---

### Use Case 2: OAuth Provider Addition

**The Problem:** Adding a new OAuth provider required 2-4 hours of planning (researching existing patterns, identifying files, estimating effort) plus 8-12 hours of implementation.

**The Solution:** Phase 2 found two similar past implementations (Google OAuth, GitHub OAuth), extracted lessons learned (token refresh edge cases, multi-tenant needs), and generated an accurate plan. Phase 3 implemented it.

**Outcome:** Planning time dropped from 4 hours to 15 minutes. Estimates improved from Â±50% to Â±20% accuracy. Implementation from plan: 1.5 hours vs 10 hours.

---

### Use Case 3: Security Review at Scale

**The Problem:** A team generating 40 PRs/month spent 2-4 hours per review. A race condition in token refresh logic slipped through manual review and caused a production incident.

**The Solution:** Phase 4 code review agent catches security risks, race conditions, and edge cases in 5 minutes per PR. Human reviewer reads agent analysis (15 min) instead of entire diff (3 hours).

**Outcome:** Review time: 3 hours â†’ 20 minutes. The same race condition pattern was flagged automatically in the next PR. Zero security incidents in following quarter.

---

## âœ… What You Can Do Today

**Immediate Actions (15 minutes):**
- [ ] Review the 4 workflow files in this talk's repository directory
- [ ] Identify which of your repositories would benefit most (highest issue volume)
- [ ] Check prerequisites: GitHub Actions enabled, Copilot license active

**Short-Term Implementation (2-3 hours):**
- [ ] Copy [`1-issue-triage.yml`](1-issue-triage.yml) to your repo's `.github/workflows/`
- [ ] Create externalized prompt templates in `.github/prompts/`
- [ ] Configure `COPILOT_GITHUB_TOKEN` secret with appropriate scopes
- [ ] Create a test issue and verify the triage workflow runs

**Advanced Exploration (1-2 weeks):**
- [ ] Add Phases 2-4 progressively (one per week)
- [ ] Customize prompt templates for your team's coding standards
- [ ] Track metrics: triage time, plan accuracy, review speed
- [ ] Monitor for [Phase 5 graduation signals](#phase-5-when-to-graduate-to-full-sdlc)

**Next Steps After Completion:**
1. âœ… Complete Phase 1 deployment and validate for 1-2 weeks
2. ğŸ“– Review [Agentic SDLC](../agentic-sdlc/) for full transformation planning
3. ğŸ“Š Build ROI dashboard from metrics to justify expansion
4. ğŸš€ Present results to leadership using [Agentic Delivery](../../exec-talks/agentic-delivery/) framing

---

## Expected ROI by Phase

For a team processing 50 issues/month:

| Phase | Setup Time | Monthly Savings | Annual Savings |
|-------|-----------|-----------------|----------------|
| **Phase 1: Intake** | 2-3 hours | $2,000 | $24,000 |
| **Phase 2: Planning** | 2-3 hours | $7,000 | $84,000 |
| **Phase 3: Coding** | 0 hours | $17,000 | $204,000 |
| **Phase 4: Review** | 1-2 hours | $10,000 | $120,000 |
| **Combined Phases 1-4** | ~8 hours | **$36,000** | **$432,000** |

*Based on $100/hour developer cost. Phase 5 (full SDLC) adds $1.8M/year but requires 3-6 month investment.*

---

## Related Patterns

### Complementary Features

- **[Agentic SDLC](../agentic-sdlc/)** â€” When you graduate beyond Phases 1-4: repository topology, PR workflows, CI trust factory
- **[Agent Teams](../agent-teams/)** â€” Specialized agent patterns for complex multi-agent coordination
- **[Copilot Hooks](../copilot-hooks/)** â€” Governance and control mechanisms for agent behavior

### Decision Flow

**If this talk doesn't fit your needs:**

```
Q: What's your actual goal?
â”œâ”€ Full SDLC transformation â†’ See: Agentic SDLC
â”œâ”€ Interactive dev sessions â†’ See: Agentic Sessions
â”œâ”€ Organization-wide adoption â†’ See: Enterprise Patterns
â””â”€ Executive framing â†’ See: Agentic Delivery (exec-talks)
```

See [DECISION-GUIDE.md](../DECISION-GUIDE.md) for complete navigation help.

---

## ğŸ“š Official Documentation

**Primary Documentation:**
- ğŸ“– **[GitHub Copilot Documentation](https://docs.github.com/en/copilot)** â€” Core concepts, getting started guide, and overview
- ğŸ“– **[GitHub Actions Workflow Syntax](https://docs.github.com/en/actions/reference/workflow-syntax-for-github-actions)** â€” Triggers, jobs, steps, and permissions
- ğŸ“– **[Copilot for Pull Requests](https://github.com/features/copilot/pull-requests)** â€” Agent-assisted PR workflows and review

**Additional Resources:**
- ğŸ“ [GitHub Copilot CLI Guide](https://docs.github.com/en/copilot/using-github-copilot/using-github-copilot-in-the-command-line) â€” CLI installation and programmatic mode
- ğŸ”§ [GitHub Actions Secrets](https://docs.github.com/en/actions/security-guides/encrypted-secrets) â€” Managing tokens and authentication

**GitHub Resources:**
- ğŸ™ [Workflow Files in This Repository](.) â€” Working reference implementations
- ğŸ“‹ [GitHub Copilot Changelog](https://github.blog/changelog/label/copilot/) â€” Latest updates and capabilities

---

## ğŸ­ Behind the Scenes

### Label-Based State Machine

The 4-workflow architecture uses GitHub issue labels as a state machine:

| Label | Set By | Triggers | Meaning |
|-------|--------|----------|---------|
| `status:triaged` | Workflow 1 | Workflow 2 | Issue analyzed, ready for planning |
| `status:planned` | Workflow 2 | Workflow 3 (via `/approve-plan`) | Plan generated, awaiting human approval |
| `status:in-review` | Workflow 3 | Workflow 4 | Implementation complete, PR created |
| `status:reviewed` | Workflow 4 | Human approval | Automated review complete |

**Why labels, not direct triggers?** Labels are observable (anyone can see pipeline state), debuggable (re-add a label to re-trigger), and human-controllable (remove a label to stop the pipeline).

### Copilot CLI Programmatic Mode

All workflows use `copilot -p` (programmatic mode) â€” the `-p` flag is **critical** for non-interactive CI/CD execution:

```bash
copilot -p @instructions.txt \
  --allow-tool 'shell(gh)' \
  --allow-tool 'shell(git)' > output.txt
```

- `@file` syntax loads external instructions
- `--allow-tool` grants permission for specific shell commands
- Output redirection captures structured responses for parsing
- `COPILOT_GITHUB_TOKEN` authenticates via environment variable

**Why This Matters:** Understanding the programmatic mode is essential for debugging workflow failures. Common issues: missing `-p` flag (opens interactive mode), insufficient token scopes, and prompt template variable substitution errors.

### Why Externalized Prompts Matter

Embedding instructions in YAML creates a maintenance nightmare â€” you can't diff prompt changes easily, non-technical stakeholders can't review them, and iterating requires workflow file changes that may trigger unrelated CI.

Externalized prompts in `.github/prompts/*.md` solve all three problems. The `sed` variable substitution pattern is intentionally simple â€” no templating engine required, works in any shell, and the substitution is visible in workflow logs for debugging.
