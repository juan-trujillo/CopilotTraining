# AI-Generated Code Review Guide

> **Purpose**: Comprehensive checklist for reviewing code generated by GitHub Copilot or other AI tools.  
> **Tier**: ðŸ†“ Free (works with any tier)  
> **Usage**: Reference during code review, add to repository as `COPILOT-REVIEW-GUIDE.md`

---

## Quick Reference Checklist

```
â–¡ Correctness    - Does it actually solve the problem?
â–¡ Security       - Any vulnerabilities introduced?
â–¡ Performance    - Efficient algorithms and patterns?
â–¡ Maintainability - Can the team understand and modify it?
â–¡ Testing        - Adequate coverage for the changes?
â–¡ Edge Cases     - Boundary conditions handled?
```

---

## Detailed Review Categories

### 1. ðŸŽ¯ Correctness & Intent

**The AI might generate plausible-looking code that doesn't actually solve the problem.**

#### Questions to Ask
- [ ] Does the code match the original requirement/ticket?
- [ ] Are there any logical errors in the implementation?
- [ ] Does the algorithm handle all expected inputs correctly?
- [ ] Are the return types and values correct?
- [ ] Does it integrate properly with existing code?

#### Common AI Mistakes
- Implementing similar-but-different functionality
- Missing edge cases in business logic
- Off-by-one errors in loops and indices
- Incorrect operator precedence
- Type coercion issues

#### Review Technique
```
1. Read the original requirement
2. Trace through the code manually with sample inputs
3. Verify the output matches expectations
4. Check integration points with existing code
```

---

### 2. ðŸ”’ Security

**AI models are trained on code that may contain vulnerabilities. Always verify security.**

#### Questions to Ask
- [ ] Is user input validated and sanitized?
- [ ] Are there any SQL injection vulnerabilities?
- [ ] Is sensitive data properly protected?
- [ ] Are authentication/authorization checks in place?
- [ ] Are there any hardcoded secrets or credentials?

#### Security Checklist

**Input Validation**
```
â–¡ All user inputs validated
â–¡ Input length limits enforced
â–¡ Special characters escaped/sanitized
â–¡ File upload restrictions (type, size)
```

**Data Protection**
```
â–¡ Sensitive data encrypted at rest
â–¡ TLS for data in transit
â–¡ No PII in logs
â–¡ Secrets from secure storage only
```

**Authentication & Authorization**
```
â–¡ Auth required for protected endpoints
â–¡ Role checks before sensitive operations
â–¡ Session management secure
â–¡ CSRF protection in place
```

**Common Vulnerabilities**
```
â–¡ No SQL injection (parameterized queries)
â–¡ No XSS (output encoding)
â–¡ No path traversal (validate file paths)
â–¡ No command injection (avoid shell execution)
```

---

### 3. âš¡ Performance

**AI may generate correct but inefficient code.**

#### Questions to Ask
- [ ] Is the algorithm complexity appropriate?
- [ ] Are there unnecessary database queries (N+1)?
- [ ] Is caching used where appropriate?
- [ ] Are there potential memory leaks?
- [ ] Will this scale with expected data volumes?

#### Performance Red Flags
- Nested loops over large datasets â†’ O(nÂ²) or worse
- Database queries inside loops
- Loading entire datasets when pagination is possible
- Synchronous operations that should be async
- Missing indexes for frequent queries

#### Review Technique
```
1. Identify the hot path (most frequently executed code)
2. Estimate time/space complexity
3. Look for database queries in loops
4. Check for missing pagination/limits
5. Verify async patterns where appropriate
```

---

### 4. ðŸ§¹ Maintainability

**Code will be read many more times than it's written.**

#### Questions to Ask
- [ ] Is the code readable without excessive comments?
- [ ] Are variable/function names descriptive?
- [ ] Is the code properly structured and organized?
- [ ] Does it follow team coding standards?
- [ ] Are there any "magic numbers" that should be constants?

#### Maintainability Checklist
```
â–¡ Clear, intention-revealing names
â–¡ Functions are single-purpose and small
â–¡ No deeply nested conditionals
â–¡ Constants instead of magic values
â–¡ Consistent formatting
â–¡ Appropriate comments (why, not what)
```

---

### 5. ðŸ§ª Testing

**AI-generated tests may look comprehensive but miss critical scenarios.**

#### Questions to Ask
- [ ] Are there tests for the happy path?
- [ ] Are edge cases and error conditions tested?
- [ ] Do tests actually assert meaningful behavior?
- [ ] Are the tests independent and repeatable?
- [ ] Is the test data realistic?

#### Test Review Checklist
```
â–¡ Tests cover happy path
â–¡ Tests cover error conditions
â–¡ Tests cover boundary values
â–¡ Tests are isolated (no shared state)
â–¡ Assertions are meaningful (not just "no error")
â–¡ Test names describe the scenario
â–¡ No flaky tests (time-dependent, order-dependent)
```

#### Common AI Testing Mistakes
- Tests that pass but don't actually verify behavior
- Missing error/exception handling tests
- Overly coupled tests that break together
- Hard-coded test data that doesn't represent production
- Tests that test the framework, not the code

---

### 6. ðŸ”„ Edge Cases

**AI often generates code that works for typical cases but fails at boundaries.**

#### Critical Edge Cases to Verify

**Numeric Values**
```
â–¡ Zero
â–¡ Negative numbers
â–¡ Very large numbers (overflow)
â–¡ Floating point precision
â–¡ NaN and Infinity
```

**Strings**
```
â–¡ Empty string
â–¡ Very long strings
â–¡ Unicode/special characters
â–¡ Whitespace only
â–¡ Null vs empty
```

**Collections**
```
â–¡ Empty collection
â–¡ Single element
â–¡ Very large collections
â–¡ Null elements
â–¡ Duplicate elements
```

**Time & Dates**
```
â–¡ Timezone handling
â–¡ Daylight saving transitions
â–¡ Leap years
â–¡ Invalid dates
â–¡ Date ranges spanning boundaries
```

**Concurrency**
```
â–¡ Race conditions
â–¡ Deadlock potential
â–¡ Thread safety
â–¡ Atomic operations where needed
```

---

## Review Workflow

### Before You Start
1. Understand the original requirement
2. Note which code was AI-generated vs human-written
3. Check if there are existing patterns to follow

### During Review
1. Read through once for overall understanding
2. Use this checklist systematically
3. Test locally if changes are significant
4. Note questions for the author

### Providing Feedback
- Be specific about concerns
- Reference this guide when applicable
- Distinguish blocking issues from suggestions
- Acknowledge what's done well

---

## Quick Decision Tree

```
Is the code correct?
â”œâ”€â”€ No â†’ Request changes (blocking)
â””â”€â”€ Yes â†’ Continue
    â”‚
    Is it secure?
    â”œâ”€â”€ No â†’ Request changes (blocking)
    â””â”€â”€ Yes â†’ Continue
        â”‚
        Is it performant enough?
        â”œâ”€â”€ No (critical path) â†’ Request changes
        â”œâ”€â”€ No (non-critical) â†’ Suggest improvement
        â””â”€â”€ Yes â†’ Continue
            â”‚
            Is it maintainable?
            â”œâ”€â”€ Major issues â†’ Request changes
            â”œâ”€â”€ Minor issues â†’ Suggest improvement
            â””â”€â”€ Yes â†’ Continue
                â”‚
                Are tests adequate?
                â”œâ”€â”€ No â†’ Request changes
                â””â”€â”€ Yes â†’ âœ… Approve
```

---

## Related Resources

- [OWASP Top 10](https://owasp.org/www-project-top-ten/)
- [GitHub Docs: Best practices for Copilot](https://docs.github.com/en/copilot/using-github-copilot/best-practices-for-using-github-copilot)
- [Module 11: Enterprise Patterns](../../modules/11-enterprise-patterns/README.md)
